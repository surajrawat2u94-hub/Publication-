name: Sync OpenAlex â†’ institution_data.json (ROR + affiliation fallback)

on:
  workflow_dispatch: {}
  schedule:
    - cron: "0 1 * * 1"   # Mondays 01:00 UTC

jobs:
  sync:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Fetch ALL works, merge, de-dupe, write JSON
        env:
          OPENALEX_EMAIL: ${{ secrets.OPENALEX_EMAIL }}
          ROR_ID: ${{ secrets.ROR_ID }}
          DATE_FROM: ${{ secrets.DATE_FROM || '2010-01-01' }}
          AFFIL_KEYWORDS: ${{ secrets.AFFIL_KEYWORDS || 'UPES|University of Petroleum and Energy Studies|University of Petroleum & Energy Studies' }}
        run: |
          python - << 'PY'
          import json, time, sys, re, urllib.parse, urllib.request, ssl

          EMAIL = ("${{ secrets.OPENALEX_EMAIL }}").strip()
          ROR   = ("${{ secrets.ROR_ID }}").strip()
          DATE_FROM = ("${{ secrets.DATE_FROM || '2010-01-01' }}").strip()
          DATE_TO   = f"{time.gmtime().tm_year}-12-31"
          KEYWORDS  = ("${{ secrets.AFFIL_KEYWORDS || 'UPES|University of Petroleum and Energy Studies|University of Petroleum & Energy Studies' }}").strip()

          if not EMAIL: sys.exit("âŒ OPENALEX_EMAIL secret is missing")
          if not ROR:   sys.exit("âŒ ROR_ID secret is missing")

          print(f"ðŸ”§ ROR_ID: {ROR}")
          print(f"ðŸ”§ Window: {DATE_FROM} â†’ {DATE_TO}")
          print(f"ðŸ”§ Keywords: {KEYWORDS}")

          BASE = "https://api.openalex.org"
          UA = f"OpenAlex-ROR-Sync/2.0 (+mailto:{EMAIL})"

          kw_parts = [k.strip() for k in KEYWORDS.split("|") if k.strip()]
          KW_RE = re.compile("|".join([re.escape(k) for k in kw_parts]), re.IGNORECASE) if kw_parts else None

          def build_url(path, params):
            q = urllib.parse.urlencode(params, doseq=True)
            return f"{BASE}{path}?{q}"

          def polite_get(url, backoff_ms=800, tries=0):
            req = urllib.request.Request(url, headers={
              "User-Agent": UA,
              "From": EMAIL,
              "Accept": "application/json",
              "Connection": "keep-alive",
            })
            ctx = ssl.create_default_context()
            try:
              with urllib.request.urlopen(req, context=ctx, timeout=60) as r:
                return r.getcode(), dict(r.headers), r.read().decode("utf-8")
            except urllib.error.HTTPError as e:
              body = e.read().decode("utf-8", "ignore") if e.fp else ""
              status = e.code
              retry_after = int(e.headers.get("Retry-After", "0") or "0")
              if status in (403,429,500,502,503,504) and tries < 8:
                wait = max(backoff_ms, retry_after*1000)
                print(f"HTTP {status}. Backing off {wait}ms (attempt {tries+1})â€¦")
                time.sleep(wait/1000)
                return polite_get(url, min(backoff_ms*2, 12000), tries+1)
              print(f"âŒ HTTP {status}: {body[:400]}")
              return status, {}, body
            except Exception as e:
              if tries < 5:
                print(f"âŒ Network error: {e}. retryingâ€¦")
                time.sleep(backoff_ms/1000)
                return polite_get(url, backoff_ms*2, tries+1)
              raise

          def fetch_cursor(full_url, tag):
            total=0; page=0; items=[]; cursor="*"
            while True:
              page+=1
              sep = "&" if "?" in full_url else "?"
              url = f"{full_url}{sep}per-page=200&cursor={urllib.parse.quote(cursor)}&mailto={urllib.parse.quote(EMAIL)}"
              code, headers, body = polite_get(url)
              if code != 200: sys.exit(1)
              j = json.loads(body)
              if page == 1:
                mc = (j.get("meta") or {}).get("count")
                print(f"[{tag}] meta.count = {mc}")
                print(f"[{tag}] first URL: {url}")
              res = j.get("results") or []
              if not res:
                if page == 1: print(f"[{tag}] No results on page 1")
                break
              items.extend(res); total += len(res)
              cursor = (j.get("meta") or {}).get("next_cursor")
              print(f"[{tag}] page {page} +{len(res)} â†’ {total}")
              if not cursor: break
              time.sleep(0.2)
            return items

          # ---------- 1) Resolve institution via ROR, get works_api_url ----------
          inst = None
          for ror_value in (ROR, f"https://ror.org/{ROR}"):
            url = build_url("/institutions", {"filter": f"ror:{ror_value}", "per-page": 1, "mailto": EMAIL})
            code, headers, body = polite_get(url)
            if code == 200:
              obj = json.loads(body)
              if obj.get("results"):
                inst = obj["results"][0]
                print(f"ðŸ« Resolved institution: {inst.get('display_name')} ({inst.get('id')})")
                break

          if not inst:
            # Try a search fallback by display_name using the first keyword
            if kw_parts:
              url = build_url("/institutions", {"search": kw_parts[0], "per-page": 5, "mailto": EMAIL})
              code, headers, body = polite_get(url)
              if code == 200:
                obj = json.loads(body)
                if obj.get("results"):
                  inst = obj["results"][0]
                  print(f"ðŸ« Resolved via search: {inst.get('display_name')} ({inst.get('id')})")
          if not inst:
            sys.exit("âŒ Could not resolve institution from ROR/search")

          works_url = inst.get("works_api_url")  # canonical!
          if not works_url:
            # construct using institutions.id if missing
            inst_id = inst.get("id")  # e.g., https://openalex.org/I1234
            works_url = build_url("/works", {"filter": f"authorships.institutions.id:{inst_id}"})
          # add date window
          join = "&" if "?" in works_url else "?"
          works_url = f"{works_url}{join}filter=from_publication_date:{DATE_FROM},to_publication_date:{DATE_TO}"

          # ---------- 2) PASS A â€” canonical ROR/ID pull via works_api_url ----------
          passA = fetch_cursor(works_url, "ROR/ID")

          # ---------- 3) PASS B â€” keyword fallback + client-side affiliation match ----------
          passB = []
          if kw_parts:
            seen=set()
            for s in kw_parts:
              search_url = build_url("/works", {
                "search": s,
                "filter": f"from_publication_date:{DATE_FROM},to_publication_date:{DATE_TO}"
              })
              got = fetch_cursor(search_url, f"KW:{s}")
              for w in got:
                wid = w.get("id")
                if wid in seen: continue
                hit = False
                for a in (w.get("authorships") or []):
                  raw = (a.get("raw_affiliation_string") or "")
                  inst_names = " ".join([i.get("display_name","") for i in (a.get("institutions") or [])])
                  if KW_RE and (KW_RE.search(raw) or KW_RE.search(inst_names)):
                    hit = True; break
                if hit:
                  passB.append(w); seen.add(wid)
          print(f"[KW] kept after client-side filtering: {len(passB)}")

          # ---------- 4) Normalize â†’ portal shape ----------
          def norm_issns(obj):
            out=[]
            src = (obj or {}).get("primary_location", {}).get("source", {}) or {}
            issn = src.get("issn")
            if isinstance(issn, list): out.extend(issn)
            elif isinstance(issn, str): out.append(issn)
            if src.get("issn_l"): out.append(src["issn_l"])
            clean = []
            for i in out:
              i = (i or "").replace("-","").upper()
              if len(i)==8: clean.append(i[:4]+"-"+i[4:])
            return sorted(set(clean))

          def shape(w):
            doi = w.get("doi")
            url = f"https://doi.org/{doi}" if doi else (w.get("primary_location") or {}).get("landing_page_url") \
                  or (w.get("primary_location") or {}).get("pdf_url") or ""
            return {
              "id": w.get("id"),
              "title": w.get("display_name"),
              "journal": ((w.get("primary_location") or {}).get("source") or {}).get("display_name"),
              "year": w.get("publication_year"),
              "type": w.get("type"),
              "citations": w.get("cited_by_count") or 0,
              "cited_by_count": w.get("cited_by_count") or 0,
              "doi": doi,
              "url": url,
              "issns": norm_issns(w),
              "authorships": w.get("authorships") or [],
              "is_retracted": bool(w.get("is_retracted", False)),
              "concepts": w.get("concepts") or [],
              "subjects": [c.get("display_name") for c in (w.get("concepts") or []) if c.get("display_name")][:6],
            }

          # ---------- 5) Merge & save ----------
          by_id={}
          for w in passA + passB:
            wid = w.get("id")
            if wid and wid not in by_id:
              by_id[wid]=shape(w)

          all_items = list(by_id.values())
          all_items.sort(key=lambda x: (x.get("year") or 0, x.get("title") or ""), reverse=True)

          out = {"updated": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                 "count": len(all_items),
                 "items": all_items}
          with open("institution_data.json","w",encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False)
          print(f"ðŸ’¾ Wrote {len(all_items)} items â†’ institution_data.json")
          PY

      - name: Commit data
        run: |
          git config user.name "openalex-sync-bot"
          git config user.email "actions@users.noreply.github.com"
          git add institution_data.json
          if git diff --cached --quiet; then
            echo "No changes."
          else
            git commit -m "data: update institution_data.json (ROR + keywords)"
            git push
          fi
