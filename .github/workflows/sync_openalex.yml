name: Sync OpenAlex by ROR → institution_data.json (with keyword fallback + debug)

on:
  workflow_dispatch: {}
  schedule:
    - cron: "0 1 * * 1"

jobs:
  sync:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Fetch ALL works (ROR + affiliation string fallback), merge & write JSON
        env:
          OPENALEX_EMAIL: ${{ secrets.OPENALEX_EMAIL }}
          ROR_ID: ${{ secrets.ROR_ID }}
          DATE_FROM: ${{ secrets.DATE_FROM || '2010-01-01' }}
          AFFIL_KEYWORDS: ${{ secrets.AFFIL_KEYWORDS || 'UPES|University of Petroleum and Energy Studies|University of Petroleum & Energy Studies' }}
        run: |
          python - << 'PY'
          import json, time, sys, re, urllib.parse, urllib.request, ssl

          EMAIL = ("${{ secrets.OPENALEX_EMAIL }}").strip()
          ROR   = ("${{ secrets.ROR_ID }}").strip()
          DATE_FROM = ("${{ secrets.DATE_FROM || '2010-01-01' }}").strip()
          DATE_TO   = f"{time.gmtime().tm_year}-12-31"
          KEYWORDS  = ("${{ secrets.AFFIL_KEYWORDS || 'UPES|University of Petroleum and Energy Studies|University of Petroleum & Energy Studies' }}").strip()

          if not EMAIL:
            print("❌ OPENALEX_EMAIL secret is missing.")
            sys.exit(1)
          if not ROR:
            print("❌ ROR_ID secret is missing or blank.")
            sys.exit(1)

          print(f"🔧 Using ROR: {ROR}")
          print(f"🔧 Date window: {DATE_FROM} → {DATE_TO}")
          print(f"🔧 Keywords: {KEYWORDS}")

          BASE = "https://api.openalex.org/works"
          UA = f"OpenAlex-ROR-Sync/1.2 (+mailto:{EMAIL})"

          # compile keyword regex
          kw_parts = [k.strip() for k in KEYWORDS.split("|") if k.strip()]
          KW_RE = re.compile("|".join([re.escape(k) for k in kw_parts]), re.IGNORECASE) if kw_parts else None

          def build_url(params):
            return BASE + "?" + urllib.parse.urlencode(params, doseq=True)

          def polite_get(url, backoff_ms=800, tries=0):
            req = urllib.request.Request(url, headers={
              "User-Agent": UA,
              "From": EMAIL,
              "Accept": "application/json",
              "Connection": "keep-alive",
            })
            ctx = ssl.create_default_context()
            try:
              with urllib.request.urlopen(req, context=ctx, timeout=60) as r:
                data = r.read().decode("utf-8")
                return r.getcode(), dict(r.headers), data
            except urllib.error.HTTPError as e:
              body = e.read().decode("utf-8", "ignore") if e.fp else ""
              status = e.code
              retry_after = int(e.headers.get("Retry-After", "0") or "0")
              if status in (403, 429, 500, 502, 503, 504) and tries < 8:
                wait = max(backoff_ms, retry_after*1000)
                print(f"HTTP {status}. Backing off {wait}ms (attempt {tries+1}) and retrying…")
                time.sleep(wait/1000)
                return polite_get(url, min(backoff_ms*2, 12000), tries+1)
              print(f"❌ HTTP {status}. Body: {body[:400]}")
              return status, {}, body
            except Exception as e:
              if tries < 5:
                print(f"❌ Network error: {e}. retrying…")
                time.sleep(backoff_ms/1000)
                return polite_get(url, backoff_ms*2, tries+1)
              raise

          def norm_issns(obj):
            out=[]
            src = (obj or {}).get("primary_location", {}).get("source", {}) or {}
            issn = src.get("issn")
            if isinstance(issn, list): out.extend(issn)
            elif isinstance(issn, str): out.append(issn)
            if src.get("issn_l"): out.append(src["issn_l"])
            clean = []
            for i in out:
              i = (i or "").replace("-","").upper()
              if len(i)==8: clean.append(i[:4]+"-"+i[4:])
            return sorted(set(clean))

          def to_portal_shape(w):
            doi = w.get("doi")
            url = f"https://doi.org/{doi}" if doi else (w.get("primary_location") or {}).get("landing_page_url") \
                  or (w.get("primary_location") or {}).get("pdf_url") or ""
            return {
              "id": w.get("id"),
              "title": w.get("display_name"),
              "journal": ((w.get("primary_location") or {}).get("source") or {}).get("display_name"),
              "year": w.get("publication_year"),
              "type": w.get("type"),
              "cited_by_count": w.get("cited_by_count") or 0,
              "citations": w.get("cited_by_count") or 0,
              "doi": doi,
              "url": url,
              "issns": norm_issns(w),
              "authorships": w.get("authorships") or [],
              "is_retracted": bool(w.get("is_retracted", False)),
              "concepts": w.get("concepts") or [],
              "subjects": [c.get("display_name") for c in (w.get("concepts") or []) if c.get("display_name")][:6],
            }

          def fetch_cursor(params, tag):
            total = 0
            page = 0
            cursor = "*"
            items = []
            while True:
              page += 1
              p = dict(params)
              p["cursor"] = cursor
              p["per-page"] = "200"
              p["mailto"] = EMAIL
              url = build_url(p)
              code, headers, body = polite_get(url)
              if code != 200:
                sys.exit(1)
              j = json.loads(body)
              if page == 1:
                mc = (j.get("meta") or {}).get("count")
                print(f"[{tag}] meta.count reported by OpenAlex: {mc}")
                print(f"[{tag}] first URL: {url}")
              res = j.get("results") or []
              if not res:
                if page == 1:
                  print(f"[{tag}] No results on first page.")
                break
              items.extend(res)
              total += len(res)
              cursor = (j.get("meta") or {}).get("next_cursor")
              print(f"[{tag}] Page {page} fetched {len(res)} • total {total}")
              if not cursor:
                break
              time.sleep(0.2)
            return items

          # PASS 1 — ROR
          pass1 = fetch_cursor(
            params={
              "filter": f"institutions.ror:{ROR},from_publication_date:{DATE_FROM},to_publication_date:{DATE_TO}",
            },
            tag="ROR"
          )
          print(f"[ROR] FINAL collected: {len(pass1)}")

          # PASS 2 — keyword fallback
          pass2 = []
          if KW_RE:
            seen_ids = set()
            for s in kw_parts:
              got = fetch_cursor(
                params={
                  "search": s,
                  "filter": f"from_publication_date:{DATE_FROM},to_publication_date:{DATE_TO}",
                },
                tag=f"KW:{s}"
              )
              # client-side filter by raw affiliation string
              for w in got:
                wid = w.get("id")
                if wid in seen_ids: continue
                # raw affiliation + institution display names
                hit = False
                for a in (w.get("authorships") or []):
                  raw = (a.get("raw_affiliation_string") or "")
                  inst_names = " ".join([i.get("display_name","") for i in (a.get("institutions") or [])])
                  if KW_RE.search(raw) or KW_RE.search(inst_names):
                    hit = True; break
                if hit:
                  pass2.append(w); seen_ids.add(wid)
            print(f"[KW] FINAL collected (after client-side filtering): {len(pass2)}")

          # MERGE & DEDUPE
          by_id = {}
          for w in pass1 + pass2:
            wid = w.get("id")
            if not wid: continue
            if wid not in by_id:
              by_id[wid] = to_portal_shape(w)

          all_items = list(by_id.values())
          all_items.sort(key=lambda x: (x.get("year") or 0, x.get("title") or ""), reverse=True)

          out = {
            "updated": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "count": len(all_items),
            "items": all_items,
          }
          with open("institution_data.json","w",encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False)
          print(f"💾 Wrote {len(all_items)} items → institution_data.json")
          PY

      - name: Commit file
        run: |
          git config user.name "openalex-sync-bot"
          git config user.email "actions@users.noreply.github.com"
          git add institution_data.json
          if git diff --cached --quiet; then
            echo "No changes."
          else
            git commit -m "data: update institution_data.json (ROR + keywords + debug)"
            git push
          fi
