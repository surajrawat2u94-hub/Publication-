name: Sync OpenAlex by ROR â†’ institution_data.json (with affiliation-string fallback)

on:
  workflow_dispatch: {}
  schedule:
    - cron: "0 1 * * 1"   # every Monday 01:00 UTC (~06:30 IST)

jobs:
  sync:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Fetch ALL works (ROR + affiliation string fallback), merge & write JSON
        env:
          OPENALEX_EMAIL: ${{ secrets.OPENALEX_EMAIL }}
          ROR_ID: ${{ secrets.ROR_ID }}
          DATE_FROM: ${{ secrets.DATE_FROM || '2010-01-01' }}
          AFFIL_KEYWORDS: ${{ secrets.AFFIL_KEYWORDS || 'UPES|University of Petroleum and Energy Studies|University of Petroleum & Energy Studies' }}
        run: |
          python - << 'PY'
          import json, time, sys, re, urllib.parse, urllib.request, ssl

          EMAIL = "${{ secrets.OPENALEX_EMAIL }}".strip()
          ROR   = "${{ secrets.ROR_ID }}".strip()
          DATE_FROM = "${{ secrets.DATE_FROM || '2010-01-01' }}".strip()
          DATE_TO   = f"{time.gmtime().tm_year}-12-31"
          KEYWORDS  = "${{ secrets.AFFIL_KEYWORDS || 'UPES|University of Petroleum and Energy Studies|University of Petroleum & Energy Studies' }}".strip()

          BASE = "https://api.openalex.org/works"
          UA = f"OpenAlex-ROR-Sync/1.1 (+mailto:{EMAIL})"

          # Compile keyword matcher (case-insensitive)
          kw_parts = [k.strip() for k in KEYWORDS.split("|") if k.strip()]
          KW_RE = re.compile("|".join([re.escape(k) for k in kw_parts]), re.IGNORECASE) if kw_parts else None

          def build_url(params):
            q = urllib.parse.urlencode(params, doseq=True)
            return BASE + "?" + q

          def polite_get(url, backoff_ms=800, tries=0):
            req = urllib.request.Request(url, headers={
              "User-Agent": UA,
              "From": EMAIL,
              "Accept": "application/json",
              "Connection": "keep-alive",
            })
            ctx = ssl.create_default_context()
            try:
              with urllib.request.urlopen(req, context=ctx, timeout=60) as r:
                data = r.read().decode("utf-8")
                return r.getcode(), dict(r.headers), data
            except urllib.error.HTTPError as e:
              body = e.read().decode("utf-8", "ignore") if e.fp else ""
              status = e.code
              retry_after = int(e.headers.get("Retry-After", "0") or "0")
              if status in (403, 429, 500, 502, 503, 504) and tries < 8:
                wait = max(backoff_ms, retry_after*1000)
                print(f"HTTP {status}. Backing off {wait}ms (attempt {tries+1}) and retryingâ€¦")
                time.sleep(wait/1000)
                return polite_get(url, min(backoff_ms*2, 12000), tries+1)
              print(f"âŒ HTTP {status}. Body: {body[:400]}")
              return status, {}, body
            except Exception as e:
              if tries < 5:
                print(f"âŒ Network error: {e}. retryingâ€¦")
                time.sleep(backoff_ms/1000)
                return polite_get(url, backoff_ms*2, tries+1)
              raise

          def norm_issns(obj):
            out=[]
            src = (obj or {}).get("primary_location", {}).get("source", {}) or {}
            issn = src.get("issn")
            if isinstance(issn, list): out.extend(issn)
            elif isinstance(issn, str): out.append(issn)
            if src.get("issn_l"): out.append(src["issn_l"])
            # normalize hyphen
            clean = []
            for i in out:
              i = (i or "").replace("-","").upper()
              if len(i)==8: clean.append(i[:4]+"-"+i[4:])
            return sorted(set(clean))

          def to_portal_shape(w):
            doi = w.get("doi")
            url = f"https://doi.org/{doi}" if doi else (w.get("primary_location") or {}).get("landing_page_url") \
                  or (w.get("primary_location") or {}).get("pdf_url") or ""
            return {
              "id": w.get("id"),
              "title": w.get("display_name"),
              "journal": ((w.get("primary_location") or {}).get("source") or {}).get("display_name"),
              "year": w.get("publication_year"),
              "type": w.get("type"),
              "cited_by_count": w.get("cited_by_count") or 0,
              "citations": w.get("cited_by_count") or 0,
              "doi": doi,
              "url": url,
              "issns": norm_issns(w),
              "authorships": w.get("authorships") or [],
              "is_retracted": bool(w.get("is_retracted", False)),
              "concepts": w.get("concepts") or [],
              "subjects": [c.get("display_name") for c in (w.get("concepts") or []) if c.get("display_name")][:6],
            }

          def fetch_cursor(params, tag):
            total = 0
            page = 0
            cursor = "*"
            items = []
            while True:
              page += 1
              p = dict(params)  # copy
              p["cursor"] = cursor
              p["per-page"] = "200"
              p["mailto"] = EMAIL
              url = build_url(p)
              code, headers, body = polite_get(url)
              if code != 200:
                sys.exit(1)
              try:
                j = json.loads(body)
              except Exception as e:
                print("âŒ Invalid JSON", e)
                sys.exit(1)

              res = j.get("results") or []
              if not res:
                break
              items.extend(res)
              total += len(res)
              cursor = (j.get("meta") or {}).get("next_cursor")
              print(f"[{tag}] Page {page} fetched {len(res)} â€¢ total {total}")
              if not cursor:
                break
              time.sleep(0.2)  # polite
            return items

          # PASS 1 â€” ROR
          pass1 = fetch_cursor(
            params={
              "filter": f"institutions.ror:{ROR},from_publication_date:{DATE_FROM},to_publication_date:{DATE_TO}",
              # (no select â€” use defaults so we keep authorships/concepts)
            },
            tag="ROR"
          )

          # PASS 2 â€” keyword fallback (broad search), then client-side filter on raw_affiliation_string
          pass2_all = []
          if KW_RE:
            # We do multiple search queries to improve recall. 
            searches = kw_parts
            for s in searches:
              p = {
                "search": s,
                "filter": f"from_publication_date:{DATE_FROM},to_publication_date:{DATE_TO}",
              }
              got = fetch_cursor(params=p, tag=f"KW:{s}")
              pass2_all.extend(got)

            # client-side filter to keep only works where ANY authorship.raw_affiliation_string matches keywords
            def keep_by_affil(w):
              for a in (w.get("authorships") or []):
                raw = (a.get("raw_affiliation_string") or "") + " " + " ".join([i.get("display_name","") for i in (a.get("institutions") or [])])
                if KW_RE.search(raw):
                  return True
              return False
            pass2 = [w for w in pass2_all if keep_by_affil(w)]
          else:
            pass2 = []

          # MERGE & DEDUPE by 'id'
          by_id = {}
          for w in pass1 + pass2:
            wid = w.get("id")
            if not wid: continue
            if wid not in by_id:
              by_id[wid] = to_portal_shape(w)

          all_items = list(by_id.values())
          all_items.sort(key=lambda x: (x.get("year") or 0, x.get("title") or ""), reverse=True)

          out = {
            "updated": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "count": len(all_items),
            "items": all_items,
          }
          with open("institution_data.json","w",encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False)
          print(f"ðŸ’¾ Wrote {len(all_items)} items â†’ institution_data.json")
          PY

      - name: Commit file
        run: |
          git config user.name "openalex-sync-bot"
          git config user.email "actions@users.noreply.github.com"
          git add institution_data.json
          if git diff --cached --quiet; then
            echo "No changes."
          else
            git commit -m "chore(data): update institution_data.json from OpenAlex (ROR + keywords)"
            git push
          fi
